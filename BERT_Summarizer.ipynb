{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of BERT_Summarizer1.ipynb","provenance":[{"file_id":"1hydXxLZQsN0HWOxdjEezVmRqIaaSdzvO","timestamp":1651368073446}],"mount_file_id":"1nIQFL7uTnZJvVBJAuoz59bnhDbwCqzes","authorship_tag":"ABX9TyOxXvxGVXEsZuG2GIEJxFST"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip uninstall numpy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t1JgIxvoAHGL","executionInfo":{"status":"ok","timestamp":1651692510417,"user_tz":240,"elapsed":3767,"user":{"displayName":"Vishnu Bharadwaj","userId":"07733155317863080011"}},"outputId":"69a15b5e-0185-42e5-bc60-35e8957fe85d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: numpy 1.14.6\n","Uninstalling numpy-1.14.6:\n","  Would remove:\n","    /usr/local/bin/f2py\n","    /usr/local/lib/python3.7/dist-packages/numpy-1.14.6.dist-info/*\n","    /usr/local/lib/python3.7/dist-packages/numpy/*\n","Proceed (y/n)? y\n","  Successfully uninstalled numpy-1.14.6\n"]}]},{"cell_type":"code","source":["!pip install numpy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":322},"id":"4ZDMY_rUAIxf","executionInfo":{"status":"ok","timestamp":1651692526316,"user_tz":240,"elapsed":13944,"user":{"displayName":"Vishnu Bharadwaj","userId":"07733155317863080011"}},"outputId":"c0f40ae8-9d08-497e-ab69-ad6c0b5035a7"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting numpy\n","  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n","\u001b[K     |████████████████████████████████| 15.7 MB 393 kB/s \n","\u001b[?25hInstalling collected packages: numpy\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n","mxnet 1.4.0 requires numpy<1.15.0,>=1.8.2, but you have numpy 1.21.6 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n","bert-embedding 1.0.1 requires numpy==1.14.6, but you have numpy 1.21.6 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed numpy-1.21.6\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]}}},"metadata":{}}]},{"cell_type":"code","source":["!pip install bert-embedding"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"jc-wp0NX_5bh","executionInfo":{"status":"ok","timestamp":1651692483395,"user_tz":240,"elapsed":27839,"user":{"displayName":"Vishnu Bharadwaj","userId":"07733155317863080011"}},"outputId":"562a0f7b-d4f3-4ddd-f2cb-ef492718e5f3"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting bert-embedding\n","  Downloading bert_embedding-1.0.1-py3-none-any.whl (13 kB)\n","Collecting typing==3.6.6\n","  Downloading typing-3.6.6-py3-none-any.whl (25 kB)\n","Collecting mxnet==1.4.0\n","  Downloading mxnet-1.4.0-py2.py3-none-manylinux1_x86_64.whl (29.6 MB)\n","\u001b[K     |████████████████████████████████| 29.6 MB 2.7 MB/s \n","\u001b[?25hCollecting gluonnlp==0.6.0\n","  Downloading gluonnlp-0.6.0.tar.gz (209 kB)\n","\u001b[K     |████████████████████████████████| 209 kB 61.0 MB/s \n","\u001b[?25hCollecting numpy==1.14.6\n","  Downloading numpy-1.14.6-cp37-cp37m-manylinux1_x86_64.whl (13.8 MB)\n","\u001b[K     |████████████████████████████████| 13.8 MB 43.3 MB/s \n","\u001b[?25hCollecting graphviz<0.9.0,>=0.8.1\n","  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet==1.4.0->bert-embedding) (2.23.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert-embedding) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert-embedding) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert-embedding) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert-embedding) (2.10)\n","Building wheels for collected packages: gluonnlp\n","  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gluonnlp: filename=gluonnlp-0.6.0-py3-none-any.whl size=259930 sha256=e9a54676201bb44348951230533c8838afcd76d94abc7825a8331f91ab97099e\n","  Stored in directory: /root/.cache/pip/wheels/a6/41/8f/45bd1c58055d87aee5a71b6756a427ea8d92e506b3a9d17370\n","Successfully built gluonnlp\n","Installing collected packages: numpy, graphviz, typing, mxnet, gluonnlp, bert-embedding\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.21.6\n","    Uninstalling numpy-1.21.6:\n","      Successfully uninstalled numpy-1.21.6\n","  Attempting uninstall: graphviz\n","    Found existing installation: graphviz 0.10.1\n","    Uninstalling graphviz-0.10.1:\n","      Successfully uninstalled graphviz-0.10.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n","yellowbrick 1.4 requires numpy>=1.16.0, but you have numpy 1.14.6 which is incompatible.\n","xarray 0.18.2 requires numpy>=1.17, but you have numpy 1.14.6 which is incompatible.\n","tifffile 2021.11.2 requires numpy>=1.15.1, but you have numpy 1.14.6 which is incompatible.\n","tensorflow 2.8.0 requires numpy>=1.20, but you have numpy 1.14.6 which is incompatible.\n","tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.14.6 which is incompatible.\n","spacy 2.2.4 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n","seaborn 0.11.2 requires numpy>=1.15, but you have numpy 1.14.6 which is incompatible.\n","scikit-image 0.18.3 requires numpy>=1.16.5, but you have numpy 1.14.6 which is incompatible.\n","pywavelets 1.3.0 requires numpy>=1.17.3, but you have numpy 1.14.6 which is incompatible.\n","pymc3 3.11.4 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n","pyerfa 2.0.0.1 requires numpy>=1.17, but you have numpy 1.14.6 which is incompatible.\n","pyarrow 6.0.1 requires numpy>=1.16.6, but you have numpy 1.14.6 which is incompatible.\n","plotnine 0.6.0 requires numpy>=1.16.0, but you have numpy 1.14.6 which is incompatible.\n","pandas 1.3.5 requires numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\", but you have numpy 1.14.6 which is incompatible.\n","numba 0.51.2 requires numpy>=1.15, but you have numpy 1.14.6 which is incompatible.\n","librosa 0.8.1 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n","kapre 0.3.7 requires numpy>=1.18.5, but you have numpy 1.14.6 which is incompatible.\n","jaxlib 0.3.7+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.14.6 which is incompatible.\n","jax 0.3.8 requires numpy>=1.19, but you have numpy 1.14.6 which is incompatible.\n","imgaug 0.2.9 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n","fbprophet 0.7.1 requires numpy>=1.15.4, but you have numpy 1.14.6 which is incompatible.\n","fastai 1.0.61 requires numpy>=1.15, but you have numpy 1.14.6 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n","cvxpy 1.0.31 requires numpy>=1.15, but you have numpy 1.14.6 which is incompatible.\n","blis 0.4.1 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n","astropy 4.3.1 requires numpy>=1.17, but you have numpy 1.14.6 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed bert-embedding-1.0.1 gluonnlp-0.6.0 graphviz-0.8.4 mxnet-1.4.0 numpy-1.14.6 typing-3.6.6\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy","typing"]}}},"metadata":{}}]},{"cell_type":"code","source":["!pip install rouge_score"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VWsh2n-a_QkX","executionInfo":{"status":"ok","timestamp":1651692542449,"user_tz":240,"elapsed":7108,"user":{"displayName":"Vishnu Bharadwaj","userId":"07733155317863080011"}},"outputId":"75813f1e-c879-4a32-c967-89ec873cb8fd"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting rouge_score\n","  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.21.6)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge_score) (3.2.5)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.0.0)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.15.0)\n","Installing collected packages: rouge-score\n","Successfully installed rouge-score-0.0.4\n"]}]},{"cell_type":"code","source":["import glob\n","import re\n","import pandas as pd\n","\n","\n","class DataReader:\n","    def __init__(self, path):\n","        self.path = path\n","        self.documents = {}\n","        self.summaries = {}\n","\n","    def load_dataset(self):\n","        document_files = sorted(glob.glob(self.path + \"/document*.txt\"))\n","        summary_files = sorted(glob.glob(self.path + \"/summary*.txt\"))\n","        for document_file in document_files:\n","            doc_num = re.findall(r'\\d+', document_file)[0]\n","            with open(document_file, 'r') as file:\n","                data = file.read().replace('\\n', '')\n","            self.documents[doc_num] = data\n","        for summary_file in summary_files:\n","            doc_num = re.findall(r'\\d+', summary_file)[0]\n","            with open(summary_file, 'r') as file:\n","                data = file.read().replace('\\n', '')\n","            self.summaries[doc_num] = data\n","        return self.documents, self.summaries\n","\n","    def load_dataset_from_csv(self):\n","        df = pd.read_csv(self.path,nrows = 500)\n","        return df[df.columns[1]].to_list(), df[df.columns[2]].to_list()"],"metadata":{"id":"NPmUryl2_Tj4","executionInfo":{"status":"ok","timestamp":1651692576518,"user_tz":240,"elapsed":993,"user":{"displayName":"Vishnu Bharadwaj","userId":"07733155317863080011"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["!pip install nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A6Wnh4V6Ax6G","executionInfo":{"status":"ok","timestamp":1651247368251,"user_tz":240,"elapsed":6247,"user":{"displayName":"Vishnu Bharadwaj","userId":"07733155317863080011"}},"outputId":"ce497ea5-2ca4-4b2f-8106-8223d7e69f68"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lk9FWMZD-fvj","outputId":"80ff1f25-50f6-4c92-a6b7-21f2844ef3e4","executionInfo":{"status":"ok","timestamp":1651694236771,"user_tz":240,"elapsed":1629384,"user":{"displayName":"Vishnu Bharadwaj","userId":"07733155317863080011"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","Vocab file is not found. Downloading.\n","Downloading /root/.mxnet/models/book_corpus_wiki_en_uncased-a6607397.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/vocab/book_corpus_wiki_en_uncased-a6607397.zip...\n","Downloading /root/.mxnet/models/bert_12_768_12_book_corpus_wiki_en_uncased-75cc780f.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/bert_12_768_12_book_corpus_wiki_en_uncased-75cc780f.zip...\n","0.1 %\n","0.2 %\n","0.3 %\n","0.4 %\n","0.5 %\n","0.6 %\n","0.7000000000000001 %\n","0.8 %\n","0.8999999999999999 %\n","1.0 %\n","1.0999999999999999 %\n","1.2 %\n","1.3 %\n","1.4000000000000001 %\n","1.5 %\n","1.6 %\n","1.7000000000000002 %\n","1.7999999999999998 %\n","1.9 %\n","2.0 %\n","2.1 %\n","2.1999999999999997 %\n","2.3 %\n","2.4 %\n","2.5 %\n","2.6 %\n","2.7 %\n","2.8000000000000003 %\n","2.9000000000000004 %\n","3.0 %\n","3.1 %\n","3.2 %\n","3.3000000000000003 %\n","3.4000000000000004 %\n","3.5000000000000004 %\n","3.5999999999999996 %\n","3.6999999999999997 %\n","3.8 %\n","3.9 %\n","4.0 %\n","4.1000000000000005 %\n","4.2 %\n","4.3 %\n","4.3999999999999995 %\n","4.5 %\n","4.6 %\n","4.7 %\n","4.8 %\n","4.9 %\n","5.0 %\n","5.1 %\n","5.2 %\n","5.3 %\n","5.4 %\n","5.5 %\n","5.6000000000000005 %\n","5.7 %\n","5.800000000000001 %\n","5.8999999999999995 %\n","6.0 %\n","6.1 %\n","6.2 %\n","6.3 %\n","6.4 %\n","6.5 %\n","6.6000000000000005 %\n","6.7 %\n","6.800000000000001 %\n","6.9 %\n","7.000000000000001 %\n","7.1 %\n","7.199999999999999 %\n","7.3 %\n","7.3999999999999995 %\n","7.5 %\n","7.6 %\n","7.7 %\n","7.8 %\n","7.9 %\n","8.0 %\n","8.1 %\n","8.200000000000001 %\n","8.3 %\n","8.4 %\n","8.5 %\n","8.6 %\n","8.7 %\n","8.799999999999999 %\n","8.9 %\n","9.0 %\n","9.1 %\n","9.2 %\n","9.3 %\n","9.4 %\n","9.5 %\n","9.6 %\n","9.700000000000001 %\n","9.8 %\n","9.9 %\n","10.0 %\n","10.100000000000001 %\n","10.2 %\n","10.299999999999999 %\n","10.4 %\n","10.5 %\n","10.6 %\n","10.7 %\n","10.8 %\n","10.9 %\n","11.0 %\n","11.1 %\n","11.200000000000001 %\n","11.3 %\n","11.4 %\n","11.5 %\n","11.600000000000001 %\n","11.700000000000001 %\n","11.799999999999999 %\n","11.899999999999999 %\n","12.0 %\n","12.1 %\n","12.2 %\n","12.3 %\n","12.4 %\n","12.5 %\n","12.6 %\n","12.7 %\n","12.8 %\n","12.9 %\n","13.0 %\n","13.100000000000001 %\n","13.200000000000001 %\n","13.3 %\n","13.4 %\n","13.5 %\n","13.600000000000001 %\n","13.700000000000001 %\n","13.8 %\n","13.900000000000002 %\n","14.000000000000002 %\n","14.099999999999998 %\n","14.2 %\n","14.299999999999999 %\n","14.399999999999999 %\n","14.499999999999998 %\n","14.6 %\n","14.7 %\n","14.799999999999999 %\n","14.899999999999999 %\n","15.0 %\n","15.1 %\n","15.2 %\n","15.299999999999999 %\n","15.4 %\n","15.5 %\n","15.6 %\n","15.7 %\n","15.8 %\n","15.9 %\n","16.0 %\n","16.1 %\n","16.2 %\n","16.3 %\n","16.400000000000002 %\n","16.5 %\n","16.6 %\n","16.7 %\n","16.8 %\n","16.900000000000002 %\n","17.0 %\n","17.1 %\n","17.2 %\n","17.299999999999997 %\n","17.4 %\n","17.5 %\n","17.599999999999998 %\n","17.7 %\n","17.8 %\n","17.9 %\n","18.0 %\n","18.099999999999998 %\n","18.2 %\n","18.3 %\n","18.4 %\n","18.5 %\n","18.6 %\n","18.7 %\n","18.8 %\n","18.9 %\n","19.0 %\n","19.1 %\n","19.2 %\n","19.3 %\n","19.400000000000002 %\n","19.5 %\n","19.6 %\n","19.7 %\n","19.8 %\n","19.900000000000002 %\n","20.0 %\n","20.1 %\n","20.200000000000003 %\n","20.3 %\n","20.4 %\n","20.5 %\n","20.599999999999998 %\n","20.7 %\n","20.8 %\n","20.9 %\n","21.0 %\n","21.099999999999998 %\n","21.2 %\n","21.3 %\n","21.4 %\n","21.5 %\n","21.6 %\n","21.7 %\n","21.8 %\n","21.9 %\n","22.0 %\n","22.1 %\n","22.2 %\n","22.3 %\n","22.400000000000002 %\n","22.5 %\n","22.6 %\n","22.7 %\n","22.8 %\n","22.900000000000002 %\n","23.0 %\n","23.1 %\n","23.200000000000003 %\n","23.3 %\n","23.400000000000002 %\n","23.5 %\n","23.599999999999998 %\n","23.7 %\n","23.799999999999997 %\n","23.9 %\n","24.0 %\n","24.099999999999998 %\n","24.2 %\n","24.3 %\n","24.4 %\n","24.5 %\n","24.6 %\n","24.7 %\n","24.8 %\n","24.9 %\n","25.0 %\n","25.1 %\n","25.2 %\n","25.3 %\n","25.4 %\n","25.5 %\n","25.6 %\n","25.7 %\n","25.8 %\n","25.900000000000002 %\n","26.0 %\n","26.1 %\n","26.200000000000003 %\n","26.3 %\n","26.400000000000002 %\n","26.5 %\n","26.6 %\n","26.700000000000003 %\n","26.8 %\n","26.900000000000002 %\n","27.0 %\n","27.1 %\n","27.200000000000003 %\n","27.3 %\n","27.400000000000002 %\n","27.500000000000004 %\n","27.6 %\n","27.700000000000003 %\n","27.800000000000004 %\n","27.900000000000002 %\n","28.000000000000004 %\n","28.1 %\n","28.199999999999996 %\n","28.299999999999997 %\n","28.4 %\n","28.499999999999996 %\n","28.599999999999998 %\n","28.7 %\n","28.799999999999997 %\n","28.9 %\n","28.999999999999996 %\n","29.099999999999998 %\n","29.2 %\n","29.299999999999997 %\n","29.4 %\n","29.5 %\n","29.599999999999998 %\n","29.7 %\n","29.799999999999997 %\n","29.9 %\n","30.0 %\n","30.099999999999998 %\n","30.2 %\n","30.3 %\n","30.4 %\n","30.5 %\n","30.599999999999998 %\n","30.7 %\n","30.8 %\n","30.9 %\n","31.0 %\n","31.1 %\n","31.2 %\n","31.3 %\n","31.4 %\n","31.5 %\n","31.6 %\n","31.7 %\n","31.8 %\n","31.900000000000002 %\n","32.0 %\n","32.1 %\n","32.2 %\n","32.300000000000004 %\n","32.4 %\n","32.5 %\n","32.6 %\n","32.7 %\n","32.800000000000004 %\n","32.9 %\n","33.0 %\n","33.1 %\n","33.2 %\n","33.300000000000004 %\n","33.4 %\n","33.5 %\n","33.6 %\n","33.7 %\n","33.800000000000004 %\n","33.900000000000006 %\n","34.0 %\n","34.1 %\n","34.2 %\n","34.300000000000004 %\n","34.4 %\n","34.5 %\n","34.599999999999994 %\n","34.699999999999996 %\n","34.8 %\n","34.9 %\n","35.0 %\n","35.099999999999994 %\n","35.199999999999996 %\n","35.3 %\n","35.4 %\n","35.5 %\n","35.6 %\n","35.699999999999996 %\n","35.8 %\n","35.9 %\n","36.0 %\n","36.1 %\n","36.199999999999996 %\n","36.3 %\n","36.4 %\n","36.5 %\n","36.6 %\n","36.7 %\n","36.8 %\n","36.9 %\n","37.0 %\n","37.1 %\n","37.2 %\n","37.3 %\n","37.4 %\n","37.5 %\n","37.6 %\n","37.7 %\n","37.8 %\n","37.9 %\n","38.0 %\n","38.1 %\n","38.2 %\n","38.3 %\n","38.4 %\n","38.5 %\n","38.6 %\n","38.7 %\n","38.800000000000004 %\n","38.9 %\n","39.0 %\n","39.1 %\n","39.2 %\n","39.300000000000004 %\n","39.4 %\n","39.5 %\n","39.6 %\n","39.7 %\n","39.800000000000004 %\n","39.900000000000006 %\n","40.0 %\n","40.1 %\n","40.2 %\n","40.300000000000004 %\n","40.400000000000006 %\n","40.5 %\n","40.6 %\n","40.699999999999996 %\n","40.8 %\n","40.9 %\n","41.0 %\n","41.099999999999994 %\n","41.199999999999996 %\n","41.3 %\n","41.4 %\n","41.5 %\n","41.6 %\n","41.699999999999996 %\n","41.8 %\n","41.9 %\n","42.0 %\n","42.1 %\n","42.199999999999996 %\n","42.3 %\n","42.4 %\n","42.5 %\n","42.6 %\n","42.699999999999996 %\n","42.8 %\n","42.9 %\n","43.0 %\n","43.1 %\n","43.2 %\n","43.3 %\n","43.4 %\n","43.5 %\n","43.6 %\n","43.7 %\n","43.8 %\n","43.9 %\n","44.0 %\n","44.1 %\n","44.2 %\n","44.3 %\n","44.4 %\n","44.5 %\n","44.6 %\n","44.7 %\n","44.800000000000004 %\n","44.9 %\n","45.0 %\n","45.1 %\n","45.2 %\n","45.300000000000004 %\n","45.4 %\n","45.5 %\n","45.6 %\n","45.7 %\n","45.800000000000004 %\n","45.9 %\n","46.0 %\n","46.1 %\n","46.2 %\n","46.300000000000004 %\n","46.400000000000006 %\n","46.5 %\n","46.6 %\n","46.7 %\n","46.800000000000004 %\n","46.9 %\n","47.0 %\n","47.099999999999994 %\n","47.199999999999996 %\n","47.3 %\n","47.4 %\n","47.5 %\n","47.599999999999994 %\n","47.699999999999996 %\n","47.8 %\n","47.9 %\n","48.0 %\n","48.1 %\n","48.199999999999996 %\n","48.3 %\n","48.4 %\n","48.5 %\n","48.6 %\n","48.699999999999996 %\n","48.8 %\n","48.9 %\n","49.0 %\n","49.1 %\n","49.2 %\n","49.3 %\n","49.4 %\n","49.5 %\n","49.6 %\n","49.7 %\n","49.8 %\n","49.9 %\n","50.0 %\n","Avg rouge 1 score 0.5665610972553764\n","Avg rouge 2 score 0.20815176158735782\n","Avg rouge L score 0.36326310319317806\n"]}],"source":["#Shortcuts simplify My Drive … \n","#In the coming weeks, items in more than one folder will be replaced by shortcuts. Access to files and folders won't change.Learn more\n","import string\n","\n","import numpy as np\n","import nltk\n","#from gensim.models import Word2Vec\n","from nltk.corpus import stopwords\n","from nltk.tokenize import sent_tokenize as nlkt_sent_tokenize\n","from nltk.tokenize import word_tokenize as nlkt_word_tokenize\n","from rouge_score import rouge_scorer\n","from scipy.spatial.distance import cosine\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","from bert_embedding import BertEmbedding\n","import csv\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","#from reader import DataReader\n","\n","\n","# Calculates cosine similarity\n","def similarity(v1, v2):\n","    score = 0.0\n","    if np.count_nonzero(v1) != 0 and np.count_nonzero(v2) != 0:\n","        score = ((1 - cosine(v1, v2)) + 1) / 2\n","    return score\n","\n","\n","def sent_tokenize(text):\n","    sents = nlkt_sent_tokenize(text)\n","    sents_filtered = []\n","    for s in sents:\n","        sents_filtered.append(s)\n","    return sents_filtered\n","\n","\n","def cleanup_sentences(text):\n","    stop_words = set(stopwords.words('english'))\n","    sentences = sent_tokenize(text)\n","    sentences_cleaned = []\n","    for sent in sentences:\n","        words = nlkt_word_tokenize(sent)\n","        words = [w for w in words if w not in string.punctuation]\n","        words = [w for w in words if not w.lower() in stop_words]\n","        words = [w.lower() for w in words]\n","        sentences_cleaned.append(\" \".join(words))\n","    return sentences_cleaned\n","\n","\n","def get_tf_idf(sentences):\n","    vectorizer = CountVectorizer()\n","    sent_word_matrix = vectorizer.fit_transform(sentences)\n","\n","    transformer = TfidfTransformer(norm=None, sublinear_tf=False, smooth_idf=False)\n","    tfidf = transformer.fit_transform(sent_word_matrix)\n","    tfidf = tfidf.toarray()\n","\n","    centroid_vector = tfidf.sum(0)\n","    centroid_vector = np.divide(centroid_vector, centroid_vector.max())\n","\n","    feature_names = vectorizer.get_feature_names_out()\n","\n","    relevant_vector_indices = np.where(centroid_vector > 0.3)[0]\n","\n","    word_list = list(np.array(feature_names)[relevant_vector_indices])\n","    return word_list\n","\n","\n","def word_vectors_cache_bert(clean_sentences, result):\n","    word_vectors = dict()\n","    for i in range(0, len(clean_sentences)):\n","      for j in range(0, len(result[i][0])):\n","        word_vectors.update({result[i][0][j]: result[i][1][j]})\n","    return word_vectors\n","\n","def build_embedding_representation_bert(words, word_vectors, result):\n","    embedding_representation = np.zeros(len(result[0][1][0]), dtype=\"float32\")\n","    word_vectors_keys = set(word_vectors.keys())\n","    #print(word_vectors_keys)\n","    count = 0\n","    for w in words:\n","        if w in word_vectors_keys:\n","            embedding_representation = embedding_representation + word_vectors[w]\n","            count += 1\n","    if count != 0:\n","       embedding_representation = np.divide(embedding_representation, count)\n","    return embedding_representation\n","\n","def summarize(text, emdedding_model):\n","    raw_sentences = sent_tokenize(text)\n","    clean_sentences = cleanup_sentences(text)\n","    centroid_words = get_tf_idf(clean_sentences)\n","    word_vectors = word_vectors_cache_bert(clean_sentences, emdedding_model)\n","    # Centroid embedding representation\n","    centroid_vector = build_embedding_representation_bert(centroid_words, word_vectors, emdedding_model)\n","    sentences_scores = []\n","    for i in range(len(clean_sentences)):\n","        scores = []\n","        words = clean_sentences[i].split()\n","\n","        # Sentence embedding representation\n","        sentence_vector = build_embedding_representation_bert(words, word_vectors, emdedding_model)\n","\n","        # Cosine similarity between sentence embedding and centroid embedding\n","        score = similarity(sentence_vector, centroid_vector)\n","        sentences_scores.append((i, raw_sentences[i], score, sentence_vector))\n","    sentence_scores_sort = sorted(sentences_scores, key=lambda el: el[2], reverse=True)\n","    # for s in sentence_scores_sort:\n","    #     print(s[0], s[1], s[2])\n","    count = 0\n","    sentences_summary = []\n","    # Handle redundancy\n","    for s in sentence_scores_sort:\n","        if count > 100:\n","            break\n","        include_flag = True\n","        for ps in sentences_summary:\n","            sim = similarity(s[3], ps[3])\n","            if sim > 0.95:\n","                include_flag = False\n","        if include_flag:\n","            sentences_summary.append(s)\n","            count += len(s[1].split())\n","\n","        sentences_summary = sorted(sentences_summary, key=lambda el: el[0], reverse=False)\n","\n","    summary = \"\\n\".join([s[1] for s in sentences_summary])\n","    return summary\n","\n","\n","if __name__ == '__main__':\n","    reader = DataReader('/content/drive/MyDrive/NLP_Project/cnn_data.csv')\n","    documents,summaries = reader.load_dataset_from_csv();\n","    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)    \n","    rouge_1_recalls = np.zeros(shape=(len(documents)))\n","    rouge_2_recalls = np.zeros(shape=(len(documents)))\n","    rouge_L_recalls = np.zeros(shape=(len(documents)))\n","    count = 0\n","    bert_embedding = BertEmbedding()\n","    summaries_out = []\n","    for doc_num in range(len(documents)):\n","      try:\n","        count+=1\n","        print((count/1000)*100, \"%\")\n","        document = documents[doc_num]\n","        summary = summaries[doc_num]\n","        clean_sentences = cleanup_sentences(document)\n","        words = []\n","        for sent in clean_sentences:\n","            words.append(nlkt_word_tokenize(sent))\n","        if len(words) > 0:\n","            model = bert_embedding(clean_sentences)\n","            generated_summary = summarize(document, model)\n","            summaries_out.append(generated_summary)\n","            scores = scorer.score(summary, generated_summary)\n","            rouge_1 = scores['rouge1'].recall\n","            rouge_2 = scores['rouge2'].recall\n","            rouge_L = scores['rougeL'].recall\n","            rouge_1_recalls[int(doc_num)] = rouge_1\n","            rouge_2_recalls[int(doc_num)] = rouge_2\n","            rouge_L_recalls[int(doc_num)] = rouge_L \n","      except Exception:\n","          pass         \n","\n","    print('Avg rouge 1 score {}'.format(np.average(rouge_1_recalls)))\n","    print('Avg rouge 2 score {}'.format(np.average(rouge_2_recalls)))\n","    print('Avg rouge L score {}'.format(np.average(rouge_L_recalls)))\n","\n","    df = pd.DataFrame(summaries_out)\n","    df.to_csv(\"/content/drive/MyDrive/NLP_Project/bert_output_500.csv\")"]},{"cell_type":"code","source":["bert_embedding = BertEmbedding()"],"metadata":{"id":"52PECsLuJVAK","executionInfo":{"status":"ok","timestamp":1651702718248,"user_tz":240,"elapsed":3386,"user":{"displayName":"Vishnu Bharadwaj","userId":"07733155317863080011"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["bert_embedding "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cAuSbJBNJ2rg","executionInfo":{"status":"ok","timestamp":1651702741393,"user_tz":240,"elapsed":131,"user":{"displayName":"Vishnu Bharadwaj","userId":"07733155317863080011"}},"outputId":"41bf0028-37b1-4ab9-d44d-7209fe93145b"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<bert_embedding.bert.BertEmbedding at 0x7f11b4ef4650>"]},"metadata":{},"execution_count":9}]}]}